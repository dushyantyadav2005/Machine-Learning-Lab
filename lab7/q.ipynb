{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "217fde8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task 1: playCricket.csv Evaluation (max_depth=3) ---\n",
      "Average Metrics (5-Fold CV) for MyID3 (max_depth=3):\n",
      "  - Avg. ACCURACY: 0.9333\n",
      "  - Avg. PRECISION: 0.9667\n",
      "  - Avg. RECALL: 0.9333\n",
      "  - Avg. F1: 0.9333\n",
      "------------------------------\n",
      "Average Metrics (5-Fold CV) for MyC45 (max_depth=3):\n",
      "  - Avg. ACCURACY: 0.6000\n",
      "  - Avg. PRECISION: 0.6222\n",
      "  - Avg. RECALL: 0.6000\n",
      "  - Avg. F1: 0.5733\n",
      "------------------------------\n",
      "\n",
      "Training on full playCricket.csv to show tree structure:\n",
      "\n",
      "--- Tree Structure for MyID3 ---\n",
      "Split on: Outlook (Categorical)\n",
      "  If == Overcast:\n",
      "    Predict: Yes\n",
      "  If == Rain:\n",
      "    Split on: Wind (Categorical)\n",
      "      If == Strong:\n",
      "        Predict: No\n",
      "      If == Weak:\n",
      "        Predict: Yes\n",
      "  If == Sunny:\n",
      "    Split on: Humidity (Categorical)\n",
      "      If == High:\n",
      "        Predict: No\n",
      "      If == Normal:\n",
      "        Predict: Yes\n",
      "------------------------------\n",
      "\n",
      "--- Tree Structure for MyC45 ---\n",
      "Split on: Outlook (Categorical)\n",
      "  If == Overcast:\n",
      "    Predict: Yes\n",
      "  If == Rain:\n",
      "    Split on: Wind (Categorical)\n",
      "      If == Strong:\n",
      "        Predict: No\n",
      "      If == Weak:\n",
      "        Predict: Yes\n",
      "  If == Sunny:\n",
      "    Split on: Humidity (Categorical)\n",
      "      If == High:\n",
      "        Predict: No\n",
      "      If == Normal:\n",
      "        Predict: Yes\n",
      "------------------------------\n",
      "\n",
      "--- Task 2: drug_200.csv Evaluation (max_depth=4) ---\n",
      "Average Metrics (5-Fold CV) for MyID3 (max_depth=4):\n",
      "  - Avg. ACCURACY: 0.9050\n",
      "  - Avg. PRECISION: 0.9182\n",
      "  - Avg. RECALL: 0.9050\n",
      "  - Avg. F1: 0.9031\n",
      "------------------------------\n",
      "Average Metrics (5-Fold CV) for MyC45 (max_depth=4):\n",
      "  - Avg. ACCURACY: 0.5200\n",
      "  - Avg. PRECISION: 0.4225\n",
      "  - Avg. RECALL: 0.5200\n",
      "  - Avg. F1: 0.4446\n",
      "------------------------------\n",
      "\n",
      "Training on full drug_200.csv to show tree structure (max_depth=4):\n",
      "\n",
      "--- Tree Structure for MyID3 ---\n",
      "Split on: Na_to_K (Categorical)\n",
      "  If == False:\n",
      "    Split on: BP (Categorical)\n",
      "      If == HIGH:\n",
      "        Split on: Age (Categorical)\n",
      "          If == False:\n",
      "            Predict: drugA\n",
      "          If == True:\n",
      "            Split on: Sex (Categorical)\n",
      "              If == F:\n",
      "                Predict: drugB\n",
      "              If == M:\n",
      "                Predict: drugB\n",
      "      If == LOW:\n",
      "        Split on: Cholesterol (Categorical)\n",
      "          If == HIGH:\n",
      "            Predict: drugC\n",
      "          If == NORMAL:\n",
      "            Predict: drugX\n",
      "      If == NORMAL:\n",
      "        Predict: drugX\n",
      "  If == True:\n",
      "    Split on: BP (Categorical)\n",
      "      If == HIGH:\n",
      "        Split on: Age (Categorical)\n",
      "          If == False:\n",
      "            Split on: Cholesterol (Categorical)\n",
      "              If == HIGH:\n",
      "                Predict: drugY\n",
      "              If == NORMAL:\n",
      "                Predict: drugY\n",
      "          If == True:\n",
      "            Split on: Cholesterol (Categorical)\n",
      "              If == HIGH:\n",
      "                Predict: drugY\n",
      "              If == NORMAL:\n",
      "                Predict: drugY\n",
      "      If == LOW:\n",
      "        Split on: Cholesterol (Categorical)\n",
      "          If == HIGH:\n",
      "            Split on: Age (Categorical)\n",
      "              If == False:\n",
      "                Predict: drugY\n",
      "              If == True:\n",
      "                Predict: drugY\n",
      "          If == NORMAL:\n",
      "            Split on: Sex (Categorical)\n",
      "              If == F:\n",
      "                Predict: drugY\n",
      "              If == M:\n",
      "                Predict: drugY\n",
      "      If == NORMAL:\n",
      "        Split on: Age (Categorical)\n",
      "          If == False:\n",
      "            Split on: Cholesterol (Categorical)\n",
      "              If == HIGH:\n",
      "                Predict: drugY\n",
      "              If == NORMAL:\n",
      "                Predict: drugY\n",
      "          If == True:\n",
      "            Split on: Cholesterol (Categorical)\n",
      "              If == HIGH:\n",
      "                Predict: drugY\n",
      "              If == NORMAL:\n",
      "                Predict: drugY\n",
      "------------------------------\n",
      "\n",
      "--- Tree Structure for MyC45 ---\n",
      "Split on: BP (Categorical)\n",
      "  If == HIGH:\n",
      "    Split on: Na_to_K (Categorical)\n",
      "      If == 6.269:\n",
      "        Predict: drugA\n",
      "      If == 7.49:\n",
      "        Predict: drugA\n",
      "      If == 8.011:\n",
      "        Predict: drugA\n",
      "      If == 8.621:\n",
      "        Predict: drugB\n",
      "      If == 8.7:\n",
      "        Predict: drugA\n",
      "      If == 9.445:\n",
      "        Predict: drugA\n",
      "      If == 9.475:\n",
      "        Predict: drugA\n",
      "      If == 9.567:\n",
      "        Predict: drugB\n",
      "      If == 9.664:\n",
      "        Predict: drugA\n",
      "      If == 9.677:\n",
      "        Predict: drugB\n",
      "      If == 9.849:\n",
      "        Predict: drugB\n",
      "      If == 9.945:\n",
      "        Predict: drugB\n",
      "      If == 10.189:\n",
      "        Predict: drugB\n",
      "      If == 10.292:\n",
      "        Predict: drugA\n",
      "      If == 10.403:\n",
      "        Predict: drugA\n",
      "      If == 10.446:\n",
      "        Predict: drugA\n",
      "      If == 10.977:\n",
      "        Predict: drugB\n",
      "      If == 11.009:\n",
      "        Predict: drugB\n",
      "      If == 11.198:\n",
      "        Predict: drugA\n",
      "      If == 11.227:\n",
      "        Predict: drugA\n",
      "      If == 11.262:\n",
      "        Predict: drugA\n",
      "      If == 11.326:\n",
      "        Predict: drugA\n",
      "      If == 11.34:\n",
      "        Predict: drugB\n",
      "      If == 11.343:\n",
      "        Predict: drugB\n",
      "      If == 11.871:\n",
      "        Predict: drugA\n",
      "      If == 12.307:\n",
      "        Predict: drugA\n",
      "      If == 12.495:\n",
      "        Predict: drugB\n",
      "      If == 12.766:\n",
      "        Predict: drugA\n",
      "      If == 12.854:\n",
      "        Predict: drugA\n",
      "      If == 12.856:\n",
      "        Predict: drugA\n",
      "      If == 12.894:\n",
      "        Predict: drugA\n",
      "      If == 13.091:\n",
      "        Predict: drugA\n",
      "      If == 13.303:\n",
      "        Predict: drugB\n",
      "      If == 13.313:\n",
      "        Predict: drugA\n",
      "      If == 13.934:\n",
      "        Predict: drugB\n",
      "      If == 13.935:\n",
      "        Predict: drugB\n",
      "      If == 13.967:\n",
      "        Predict: drugB\n",
      "      If == 13.972:\n",
      "        Predict: drugA\n",
      "      If == 14.239:\n",
      "        Predict: drugB\n",
      "      If == 15.156:\n",
      "        Predict: drugY\n",
      "      If == 15.436:\n",
      "        Predict: drugY\n",
      "      If == 15.49:\n",
      "        Predict: drugY\n",
      "      If == 15.516:\n",
      "        Predict: drugY\n",
      "      If == 16.347:\n",
      "        Predict: drugY\n",
      "      If == 16.725:\n",
      "        Predict: drugY\n",
      "      If == 17.069:\n",
      "        Predict: drugY\n",
      "      If == 17.206:\n",
      "        Predict: drugY\n",
      "      If == 18.295:\n",
      "        Predict: drugY\n",
      "      If == 18.348:\n",
      "        Predict: drugY\n",
      "      If == 18.457:\n",
      "        Predict: drugY\n",
      "      If == 18.703:\n",
      "        Predict: drugY\n",
      "      If == 18.809:\n",
      "        Predict: drugY\n",
      "      If == 18.991:\n",
      "        Predict: drugY\n",
      "      If == 19.007:\n",
      "        Predict: drugY\n",
      "      If == 19.161:\n",
      "        Predict: drugY\n",
      "      If == 19.199:\n",
      "        Predict: drugY\n",
      "      If == 19.416:\n",
      "        Predict: drugY\n",
      "      If == 20.932:\n",
      "        Predict: drugY\n",
      "      If == 21.036:\n",
      "        Predict: drugY\n",
      "      If == 22.818:\n",
      "        Predict: drugY\n",
      "      If == 23.091:\n",
      "        Predict: drugY\n",
      "      If == 24.276:\n",
      "        Predict: drugY\n",
      "      If == 25.355:\n",
      "        Predict: drugY\n",
      "      If == 25.395:\n",
      "        Predict: drugY\n",
      "      If == 25.475:\n",
      "        Predict: drugY\n",
      "      If == 25.969:\n",
      "        Predict: drugY\n",
      "      If == 25.974:\n",
      "        Predict: drugY\n",
      "      If == 27.826:\n",
      "        Predict: drugY\n",
      "      If == 28.294:\n",
      "        Predict: drugY\n",
      "      If == 28.632:\n",
      "        Predict: drugY\n",
      "      If == 29.45:\n",
      "        Predict: drugY\n",
      "      If == 30.366:\n",
      "        Predict: drugY\n",
      "      If == 31.876:\n",
      "        Predict: drugY\n",
      "      If == 34.686:\n",
      "        Predict: drugY\n",
      "      If == 34.997:\n",
      "        Predict: drugY\n",
      "      If == 35.639:\n",
      "        Predict: drugY\n",
      "      If == 37.188:\n",
      "        Predict: drugY\n",
      "  If == LOW:\n",
      "    Split on: Cholesterol (Categorical)\n",
      "      If == HIGH:\n",
      "        Split on: Na_to_K (Categorical)\n",
      "          If == 6.769:\n",
      "            Predict: drugC\n",
      "          If == 7.298:\n",
      "            Predict: drugC\n",
      "          If == 8.151:\n",
      "            Predict: drugC\n",
      "          If == 9.712:\n",
      "            Predict: drugC\n",
      "          If == 10.067:\n",
      "            Predict: drugC\n",
      "          If == 10.114:\n",
      "            Predict: drugC\n",
      "          If == 10.291:\n",
      "            Predict: drugC\n",
      "          If == 10.444:\n",
      "            Predict: drugC\n",
      "          If == 10.537:\n",
      "            Predict: drugC\n",
      "          If == 11.037:\n",
      "            Predict: drugC\n",
      "          If == 11.567:\n",
      "            Predict: drugC\n",
      "          If == 11.767:\n",
      "            Predict: drugC\n",
      "          If == 12.006:\n",
      "            Predict: drugC\n",
      "          If == 13.093:\n",
      "            Predict: drugC\n",
      "          If == 13.127:\n",
      "            Predict: drugC\n",
      "          If == 14.16:\n",
      "            Predict: drugC\n",
      "          If == 15.015:\n",
      "            Predict: drugY\n",
      "          If == 15.036:\n",
      "            Predict: drugY\n",
      "          If == 15.376:\n",
      "            Predict: drugY\n",
      "          If == 15.478:\n",
      "            Predict: drugY\n",
      "          If == 16.31:\n",
      "            Predict: drugY\n",
      "          If == 17.951:\n",
      "            Predict: drugY\n",
      "          If == 18.043:\n",
      "            Predict: drugY\n",
      "          If == 18.295:\n",
      "            Predict: drugY\n",
      "          If == 19.796:\n",
      "            Predict: drugY\n",
      "          If == 20.013:\n",
      "            Predict: drugY\n",
      "          If == 20.942:\n",
      "            Predict: drugY\n",
      "          If == 22.963:\n",
      "            Predict: drugY\n",
      "          If == 26.645:\n",
      "            Predict: drugY\n",
      "          If == 33.486:\n",
      "            Predict: drugY\n",
      "          If == 38.247:\n",
      "            Predict: drugY\n",
      "      If == NORMAL:\n",
      "        Split on: Na_to_K (Categorical)\n",
      "          If == 7.34:\n",
      "            Predict: drugX\n",
      "          If == 8.37:\n",
      "            Predict: drugX\n",
      "          If == 8.968:\n",
      "            Predict: drugX\n",
      "          If == 9.17:\n",
      "            Predict: drugX\n",
      "          If == 10.017:\n",
      "            Predict: drugX\n",
      "          If == 10.84:\n",
      "            Predict: drugX\n",
      "          If == 11.014:\n",
      "            Predict: drugX\n",
      "          If == 11.349:\n",
      "            Predict: drugX\n",
      "          If == 11.424:\n",
      "            Predict: drugX\n",
      "          If == 11.455:\n",
      "            Predict: drugX\n",
      "          If == 11.686:\n",
      "            Predict: drugX\n",
      "          If == 11.939:\n",
      "            Predict: drugX\n",
      "          If == 12.006:\n",
      "            Predict: drugX\n",
      "          If == 12.923:\n",
      "            Predict: drugX\n",
      "          If == 13.598:\n",
      "            Predict: drugX\n",
      "          If == 13.769:\n",
      "            Predict: drugX\n",
      "          If == 13.938:\n",
      "            Predict: drugX\n",
      "          If == 14.642:\n",
      "            Predict: drugX\n",
      "          If == 16.724:\n",
      "            Predict: drugY\n",
      "          If == 18.739:\n",
      "            Predict: drugY\n",
      "          If == 19.128:\n",
      "            Predict: drugY\n",
      "          If == 19.368:\n",
      "            Predict: drugY\n",
      "          If == 20.693:\n",
      "            Predict: drugY\n",
      "          If == 20.909:\n",
      "            Predict: drugY\n",
      "          If == 22.697:\n",
      "            Predict: drugY\n",
      "          If == 23.003:\n",
      "            Predict: drugY\n",
      "          If == 25.741:\n",
      "            Predict: drugY\n",
      "          If == 27.183:\n",
      "            Predict: drugY\n",
      "          If == 29.271:\n",
      "            Predict: drugY\n",
      "          If == 29.875:\n",
      "            Predict: drugY\n",
      "          If == 30.568:\n",
      "            Predict: drugY\n",
      "          If == 32.922:\n",
      "            Predict: drugY\n",
      "          If == 33.542:\n",
      "            Predict: drugY\n",
      "  If == NORMAL:\n",
      "    Split on: Na_to_K (Categorical)\n",
      "      If == 6.683:\n",
      "        Predict: drugX\n",
      "      If == 7.261:\n",
      "        Predict: drugX\n",
      "      If == 7.285:\n",
      "        Predict: drugX\n",
      "      If == 7.477:\n",
      "        Predict: drugX\n",
      "      If == 7.761:\n",
      "        Predict: drugX\n",
      "      If == 7.798:\n",
      "        Predict: drugX\n",
      "      If == 7.845:\n",
      "        Predict: drugX\n",
      "      If == 8.107:\n",
      "        Predict: drugX\n",
      "      If == 8.607:\n",
      "        Predict: drugX\n",
      "      If == 8.75:\n",
      "        Predict: drugX\n",
      "      If == 8.966:\n",
      "        Predict: drugX\n",
      "      If == 9.084:\n",
      "        Predict: drugX\n",
      "      If == 9.281:\n",
      "        Predict: drugX\n",
      "      If == 9.381:\n",
      "        Predict: drugX\n",
      "      If == 9.443:\n",
      "        Predict: drugX\n",
      "      If == 9.514:\n",
      "        Predict: drugX\n",
      "      If == 9.709:\n",
      "        Predict: drugX\n",
      "      If == 9.894:\n",
      "        Predict: drugX\n",
      "      If == 10.065:\n",
      "        Predict: drugX\n",
      "      If == 10.091:\n",
      "        Predict: drugX\n",
      "      If == 10.103:\n",
      "        Predict: drugX\n",
      "      If == 10.443:\n",
      "        Predict: drugX\n",
      "      If == 10.605:\n",
      "        Predict: drugX\n",
      "      If == 10.832:\n",
      "        Predict: drugX\n",
      "      If == 10.898:\n",
      "        Predict: drugX\n",
      "      If == 11.953:\n",
      "        Predict: drugX\n",
      "      If == 12.26:\n",
      "        Predict: drugX\n",
      "      If == 12.295:\n",
      "        Predict: drugX\n",
      "      If == 12.703:\n",
      "        Predict: drugX\n",
      "      If == 12.859:\n",
      "        Predict: drugX\n",
      "      If == 12.879:\n",
      "        Predict: drugX\n",
      "      If == 13.597:\n",
      "        Predict: drugX\n",
      "      If == 13.884:\n",
      "        Predict: drugX\n",
      "      If == 14.02:\n",
      "        Predict: drugX\n",
      "      If == 14.133:\n",
      "        Predict: drugX\n",
      "      If == 14.216:\n",
      "        Predict: drugX\n",
      "      If == 15.171:\n",
      "        Predict: drugY\n",
      "      If == 15.79:\n",
      "        Predict: drugY\n",
      "      If == 15.891:\n",
      "        Predict: drugY\n",
      "      If == 15.969:\n",
      "        Predict: drugY\n",
      "      If == 16.275:\n",
      "        Predict: drugY\n",
      "      If == 16.594:\n",
      "        Predict: drugY\n",
      "      If == 16.753:\n",
      "        Predict: drugY\n",
      "      If == 16.85:\n",
      "        Predict: drugY\n",
      "      If == 17.211:\n",
      "        Predict: drugY\n",
      "      If == 17.225:\n",
      "        Predict: drugY\n",
      "      If == 19.011:\n",
      "        Predict: drugY\n",
      "      If == 19.221:\n",
      "        Predict: drugY\n",
      "      If == 19.675:\n",
      "        Predict: drugY\n",
      "      If == 20.489:\n",
      "        Predict: drugY\n",
      "      If == 22.456:\n",
      "        Predict: drugY\n",
      "      If == 22.905:\n",
      "        Predict: drugY\n",
      "      If == 24.658:\n",
      "        Predict: drugY\n",
      "      If == 25.786:\n",
      "        Predict: drugY\n",
      "      If == 25.893:\n",
      "        Predict: drugY\n",
      "      If == 25.917:\n",
      "        Predict: drugY\n",
      "      If == 27.05:\n",
      "        Predict: drugY\n",
      "      If == 27.064:\n",
      "        Predict: drugY\n",
      "      If == 31.686:\n",
      "        Predict: drugY\n",
      "------------------------------\n",
      "\n",
      "--- Task 3: petrol_consumption.csv Evaluation (max_depth=3) ---\n",
      "Average Metrics (5-Fold CV) for MyDecisionTreeRegressor (max_depth=3):\n",
      "  - Avg. MSE: 12423.4657\n",
      "  - Avg. RMSE: 106.0420\n",
      "  - Avg. MAE: 77.1072\n",
      "  - Avg. R2: -0.3347\n",
      "------------------------------\n",
      "\n",
      "Training on full petrol_consumption.csv to show tree structure (max_depth=3):\n",
      "\n",
      "--- Tree Structure for MyDecisionTreeRegressor ---\n",
      "Split on: Population_Driver_licence(%) (Threshold: 0.6675)\n",
      "  If <= 0.6675:\n",
      "    Split on: Average_income (Threshold: 4395.0000)\n",
      "      If <= 4395.0000:\n",
      "        Split on: Petrol_tax (Categorical)\n",
      "          If == 5.0:\n",
      "            Predict: 640.0000\n",
      "          If == 6.58:\n",
      "            Predict: 644.0000\n",
      "          If == 7.0:\n",
      "            Predict: 615.5833\n",
      "          If == 7.5:\n",
      "            Predict: 629.5000\n",
      "          If == 8.0:\n",
      "            Predict: 559.0000\n",
      "          If == 8.5:\n",
      "            Predict: 648.0000\n",
      "          If == 9.0:\n",
      "            Predict: 545.5000\n",
      "      If > 4395.0000:\n",
      "        Split on: Population_Driver_licence(%) (Threshold: 0.5725)\n",
      "          If <= 0.5725:\n",
      "            Predict: 450.8182\n",
      "          If > 0.5725:\n",
      "            Predict: 565.0000\n",
      "  If > 0.6675:\n",
      "    Split on: Petrol_tax (Categorical)\n",
      "      If == 6.0:\n",
      "        Predict: 782.0000\n",
      "      If == 7.0:\n",
      "        Split on: Average_income (Threshold: 4530.5000)\n",
      "          If <= 4530.5000:\n",
      "            Predict: 968.0000\n",
      "          If > 4530.5000:\n",
      "            Predict: 865.0000\n",
      "      If == 8.5:\n",
      "        Predict: 640.0000\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "\n",
    "# --- Helper Functions for Entropy/Gain/Variance ---\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"Calculates entropy of a given target array y (integer-encoded).\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    # np.bincount efficiently counts occurrences of each integer label\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * math.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def calculate_information_gain(y, subsets_y):\n",
    "    \"\"\"Calculates Information Gain.\"\"\"\n",
    "    total_len = len(y)\n",
    "    if total_len == 0:\n",
    "        return 0\n",
    "    parent_entropy = calculate_entropy(y)\n",
    "    \n",
    "    # Calculate weighted average of child node entropies\n",
    "    weighted_child_entropy = sum((len(subset) / total_len) * calculate_entropy(subset) \n",
    "                                 for subset in subsets_y if len(subset) > 0)\n",
    "    \n",
    "    return parent_entropy - weighted_child_entropy\n",
    "\n",
    "def calculate_split_info(subsets_y):\n",
    "    \"\"\"Calculates the Split Info (Intrinsic Information) for C4.5.\"\"\"\n",
    "    total_len = sum(len(subset) for subset in subsets_y)\n",
    "    if total_len == 0:\n",
    "        return 0\n",
    "    \n",
    "    split_info = 0\n",
    "    for subset in subsets_y:\n",
    "        proportion = len(subset) / total_len\n",
    "        if proportion > 0:\n",
    "            split_info -= proportion * math.log2(proportion)\n",
    "            \n",
    "    # Add a small epsilon to avoid division by zero if split_info is 0\n",
    "    return split_info + 1e-6\n",
    "\n",
    "def calculate_gain_ratio(y, subsets_y):\n",
    "    \"\"\"Calculates Gain Ratio for C4.5.\"\"\"\n",
    "    info_gain = calculate_information_gain(y, subsets_y)\n",
    "    split_info = calculate_split_info(subsets_y)\n",
    "    \n",
    "    # split_info will have 1e-6 added, so it's never zero\n",
    "    return info_gain / split_info\n",
    "\n",
    "def calculate_variance(y):\n",
    "    \"\"\"Calculates variance of a target array y (for regression).\"\"\"\n",
    "    if len(y) == 0:\n",
    "        return 0\n",
    "    return np.var(y)\n",
    "\n",
    "def calculate_variance_reduction(y, subsets_y):\n",
    "    \"\"\"Calculates Variance Reduction (like Information Gain for regression).\"\"\"\n",
    "    total_len = len(y)\n",
    "    if total_len == 0:\n",
    "        return 0\n",
    "    parent_variance = calculate_variance(y)\n",
    "    \n",
    "    # Calculate weighted average of child node variances\n",
    "    weighted_child_variance = sum((len(subset) / total_len) * calculate_variance(subset) \n",
    "                                  for subset in subsets_y if len(subset) > 0)\n",
    "    \n",
    "    return parent_variance - weighted_child_variance\n",
    "\n",
    "# --- Node and Base Tree Class ---\n",
    "\n",
    "class Node:\n",
    "    \"\"\"Decision Tree Node.\"\"\"\n",
    "    def __init__(self, feature=None, threshold=None, children=None, *, value=None, is_leaf=False):\n",
    "        self.feature = feature       # Feature index to split on\n",
    "        self.threshold = threshold   # Value/threshold for continuous split\n",
    "        self.children = children     # Dict of child nodes {'val': Node, ...}\n",
    "        self.value = value           # Prediction value (if leaf, or majority value for pruning/unseen)\n",
    "        self.is_leaf = is_leaf\n",
    "\n",
    "class MyDecisionTreeBase:\n",
    "    \"\"\"Base class for 'from scratch' decision trees.\"\"\"\n",
    "    def __init__(self, min_samples_split=2, max_depth=100):\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.root = None\n",
    "        self._feature_types = []\n",
    "        self._target_type = 'classification'\n",
    "        self.feature_names = None\n",
    "        self.label_map = {}\n",
    "        self.reverse_label_map = {}\n",
    "        self.default_value = None # For unseen categories in predict\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build the decision tree.\"\"\"\n",
    "        # Convert pandas to numpy\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "            \n",
    "        # Determine feature types\n",
    "        self._feature_types = []\n",
    "        for i in range(X.shape[1]):\n",
    "            # Treat as continuous if numeric and has > 10 unique values\n",
    "            if pd.api.types.is_numeric_dtype(X[:, i]) and len(np.unique(X[:, i])) > 10:\n",
    "                 self._feature_types.append('continuous')\n",
    "            else:\n",
    "                 self._feature_types.append('categorical')\n",
    "\n",
    "        # Determine target type and encode\n",
    "        if pd.api.types.is_numeric_dtype(y):\n",
    "            self._target_type = 'regression'\n",
    "            self.default_value = np.mean(y) # Default is mean for regression\n",
    "        else:\n",
    "            self._target_type = 'classification'\n",
    "            self.label_map = {label: i for i, label in enumerate(np.unique(y))}\n",
    "            self.reverse_label_map = {i: label for label, i in self.label_map.items()}\n",
    "            y = np.array([self.label_map[label] for label in y])\n",
    "            self.default_value = Counter(y).most_common(1)[0][0] # Default is majority class\n",
    "\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Get the value for this node (used for leaf or fallback)\n",
    "        current_leaf_value = self._get_leaf_value(y)\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (depth >= self.max_depth or\n",
    "            n_samples < self.min_samples_split or\n",
    "            len(np.unique(y)) == 1):\n",
    "            \n",
    "            return Node(value=current_leaf_value, is_leaf=True)\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "        best_split = self._find_best_split(X, y, n_features)\n",
    "\n",
    "        # If no split provides positive gain, create a leaf\n",
    "        if best_split is None or best_split['gain'] <= 0:\n",
    "            return Node(value=current_leaf_value, is_leaf=True)\n",
    "\n",
    "        feature_idx = best_split['feature_idx']\n",
    "        \n",
    "        children = {}\n",
    "        node_threshold = None\n",
    "        \n",
    "        if self._feature_types[feature_idx] == 'continuous':\n",
    "            node_threshold = best_split['threshold']\n",
    "            indices_left, indices_right = best_split['indices_left'], best_split['indices_right']\n",
    "            \n",
    "            left_node = self._build_tree(X[indices_left, :], y[indices_left], depth + 1)\n",
    "            right_node = self._build_tree(X[indices_right, :], y[indices_right], depth + 1)\n",
    "            \n",
    "            children = {\n",
    "                '<= threshold': left_node,\n",
    "                '> threshold': right_node\n",
    "            }\n",
    "        \n",
    "        else: # Categorical\n",
    "            indices_map = best_split['indices_map']\n",
    "            for value, indices in indices_map.items():\n",
    "                child_node = self._build_tree(X[indices, :], y[indices], depth + 1)\n",
    "                children[value] = child_node\n",
    "\n",
    "        # Store the majority value of this node for fallback predictions\n",
    "        return Node(feature=feature_idx, threshold=node_threshold, children=children, value=current_leaf_value)\n",
    "    \n",
    "    def _get_leaf_value(self, y):\n",
    "        \"\"\"Get the value for a leaf node.\"\"\"\n",
    "        if self._target_type == 'classification':\n",
    "            most_common = Counter(y).most_common(1)\n",
    "            # Use default value if y is empty\n",
    "            return most_common[0][0] if most_common else self.default_value\n",
    "        else:\n",
    "            # Use default value if y is empty\n",
    "            return np.mean(y) if len(y) > 0 else self.default_value\n",
    "\n",
    "    def _find_best_split(self, X, y, n_features):\n",
    "        \"\"\"Abstract method to be implemented by subclasses.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels or values for X.\"\"\"\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.values\n",
    "            \n",
    "        predictions = [self._predict_row(x, self.root) for x in X]\n",
    "        \n",
    "        if self._target_type == 'classification':\n",
    "            # Convert integer labels back to original labels\n",
    "            default_original = self.reverse_label_map.get(self.default_value)\n",
    "            return np.array([self.reverse_label_map.get(pred, default_original) for pred in predictions])\n",
    "        else:\n",
    "            return np.array(predictions)\n",
    "\n",
    "    def _predict_row(self, x, node):\n",
    "        \"\"\"Helper to predict a single row by traversing the tree.\"\"\"\n",
    "        if node.is_leaf:\n",
    "            return node.value\n",
    "\n",
    "        feature_val = x[node.feature]\n",
    "        \n",
    "        try:\n",
    "            if self._feature_types[node.feature] == 'continuous':\n",
    "                if feature_val <= node.threshold:\n",
    "                    return self._predict_row(x, node.children['<= threshold'])\n",
    "                else:\n",
    "                    return self._predict_row(x, node.children['> threshold'])\n",
    "            else: # Categorical\n",
    "                if feature_val in node.children:\n",
    "                    return self._predict_row(x, node.children[feature_val])\n",
    "                else:\n",
    "                    # Handle unseen category: return the majority value of the *current* node\n",
    "                    return node.value\n",
    "        except Exception:\n",
    "             # Fallback for any traversal error (e.g., node has no children)\n",
    "             return node.value\n",
    "\n",
    "    def print_tree(self):\n",
    "        \"\"\"Prints the decision tree structure.\"\"\"\n",
    "        if self.root is None:\n",
    "            print(\"Tree has not been fitted.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n--- Tree Structure for {self.__class__.__name__} ---\")\n",
    "        # Pass feature names and target mapping to recursive helper\n",
    "        self._print_recursive(self.root, indent=\"\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    def _print_recursive(self, node, indent):\n",
    "        \"\"\"Recursive helper for printing the tree.\"\"\"\n",
    "        if node.is_leaf:\n",
    "            # Format the leaf value\n",
    "            if self._target_type == 'classification':\n",
    "                value = self.reverse_label_map.get(node.value, \"N/A\")\n",
    "            else:\n",
    "                value = f\"{node.value:.4f}\"\n",
    "            print(f\"{indent}Predict: {value}\")\n",
    "            return\n",
    "\n",
    "        # Get feature name\n",
    "        feature_name = self.feature_names[node.feature]\n",
    "        \n",
    "        # Check if continuous or categorical\n",
    "        if node.threshold is not None:\n",
    "            # Continuous split\n",
    "            print(f\"{indent}Split on: {feature_name} (Threshold: {node.threshold:.4f})\")\n",
    "            \n",
    "            # Left child\n",
    "            print(f\"{indent}  If <= {node.threshold:.4f}:\")\n",
    "            self._print_recursive(node.children['<= threshold'], indent + \"    \")\n",
    "            \n",
    "            # Right child\n",
    "            print(f\"{indent}  If > {node.threshold:.4f}:\")\n",
    "            self._print_recursive(node.children['> threshold'], indent + \"    \")\n",
    "            \n",
    "        else:\n",
    "            # Categorical split\n",
    "            print(f\"{indent}Split on: {feature_name} (Categorical)\")\n",
    "            \n",
    "            # Sort children by value for consistent printing\n",
    "            for value, child_node in sorted(node.children.items()):\n",
    "                print(f\"{indent}  If == {value}:\")\n",
    "                self._print_recursive(child_node, indent + \"    \")\n",
    "\n",
    "\n",
    "# --- ID3 Classifier ---\n",
    "# (Assumes all features are categorical)\n",
    "class MyID3(MyDecisionTreeBase):\n",
    "    \"\"\"ID3 Decision Tree Classifier. Assumes all features are categorical.\"\"\"\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # ID3 requires all features to be categorical\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self.feature_names = X.columns\n",
    "            X = X.values\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        # Override feature types to all categorical\n",
    "        self._feature_types = ['categorical'] * X.shape[1] \n",
    "        \n",
    "        self._target_type = 'classification'\n",
    "        self.label_map = {label: i for i, label in enumerate(np.unique(y))}\n",
    "        self.reverse_label_map = {i: label for label, i in self.label_map.items()}\n",
    "        y = np.array([self.label_map[label] for label in y])\n",
    "        self.default_value = Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        self.root = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _find_best_split(self, X, y, n_features):\n",
    "        best_gain = -1\n",
    "        best_feature_idx = None\n",
    "        best_indices_map = None\n",
    "\n",
    "        # Iterate over all features\n",
    "        for feature_idx in range(n_features):\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "            \n",
    "            # Must have at least 2 unique values to split\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            subsets_y = []\n",
    "            indices_map = {}\n",
    "            for value in unique_values:\n",
    "                indices = np.where(X[:, feature_idx] == value)[0]\n",
    "                if len(indices) > 0:\n",
    "                    indices_map[value] = indices\n",
    "                    subsets_y.append(y[indices])\n",
    "\n",
    "            # If split results in less than 2 groups, skip it\n",
    "            if len(subsets_y) < 2:\n",
    "                continue\n",
    "\n",
    "            gain = calculate_information_gain(y, subsets_y)\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature_idx = feature_idx\n",
    "                best_indices_map = indices_map\n",
    "        \n",
    "        if best_gain <= 0:\n",
    "            return None\n",
    "\n",
    "        return {\n",
    "            'feature_idx': best_feature_idx,\n",
    "            'threshold': None,\n",
    "            'indices_map': best_indices_map,\n",
    "            'gain': best_gain,\n",
    "            'indices_left': None,\n",
    "            'indices_right': None\n",
    "        }\n",
    "\n",
    "# --- C4.5 Classifier ---\n",
    "# (Handles both continuous and categorical features)\n",
    "class MyC45(MyDecisionTreeBase):\n",
    "    \"\"\"C4.5 Decision Tree Classifier.\"\"\"\n",
    "\n",
    "    def _find_best_split(self, X, y, n_features):\n",
    "        best_gain_ratio = -1\n",
    "        best_split = None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_type = self._feature_types[feature_idx]\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            if feature_type == 'continuous':\n",
    "                # Find best threshold for continuous feature\n",
    "                sorted_values = np.sort(unique_values)\n",
    "                thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    indices_left = np.where(X[:, feature_idx] <= threshold)[0]\n",
    "                    indices_right = np.where(X[:, feature_idx] > threshold)[0]\n",
    "                    \n",
    "                    if len(indices_left) == 0 or len(indices_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    subsets_y = [y[indices_left], y[indices_right]]\n",
    "                    gain_ratio = calculate_gain_ratio(y, subsets_y)\n",
    "\n",
    "                    if gain_ratio > best_gain_ratio:\n",
    "                        best_gain_ratio = gain_ratio\n",
    "                        best_split = {\n",
    "                            'feature_idx': feature_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'indices_map': None,\n",
    "                            'gain': gain_ratio,\n",
    "                            'indices_left': indices_left,\n",
    "                            'indices_right': indices_right\n",
    "                        }\n",
    "            \n",
    "            else: # Categorical\n",
    "                subsets_y = []\n",
    "                indices_map = {}\n",
    "                for value in unique_values:\n",
    "                    indices = np.where(X[:, feature_idx] == value)[0]\n",
    "                    if len(indices) > 0:\n",
    "                        indices_map[value] = indices\n",
    "                        subsets_y.append(y[indices])\n",
    "                \n",
    "                if len(subsets_y) < 2:\n",
    "                    continue\n",
    "                \n",
    "                gain_ratio = calculate_gain_ratio(y, subsets_y)\n",
    "\n",
    "                if gain_ratio > best_gain_ratio:\n",
    "                    best_gain_ratio = gain_ratio\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': None,\n",
    "                        'indices_map': indices_map,\n",
    "                        'gain': gain_ratio,\n",
    "                        'indices_left': None,\n",
    "                        'indices_right': None\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "# --- Decision Tree Regressor ---\n",
    "# (Handles both continuous and categorical features)\n",
    "class MyDecisionTreeRegressor(MyDecisionTreeBase):\n",
    "    \"\"\"Decision Tree Regressor.\"\"\"\n",
    "\n",
    "    def _find_best_split(self, X, y, n_features):\n",
    "        best_var_reduction = -1\n",
    "        best_split = None\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            feature_type = self._feature_types[feature_idx]\n",
    "            unique_values = np.unique(X[:, feature_idx])\n",
    "\n",
    "            if len(unique_values) < 2:\n",
    "                continue\n",
    "\n",
    "            if feature_type == 'continuous':\n",
    "                sorted_values = np.sort(unique_values)\n",
    "                thresholds = (sorted_values[:-1] + sorted_values[1:]) / 2\n",
    "\n",
    "                for threshold in thresholds:\n",
    "                    indices_left = np.where(X[:, feature_idx] <= threshold)[0]\n",
    "                    indices_right = np.where(X[:, feature_idx] > threshold)[0]\n",
    "                    \n",
    "                    if len(indices_left) == 0 or len(indices_right) == 0:\n",
    "                        continue\n",
    "\n",
    "                    subsets_y = [y[indices_left], y[indices_right]]\n",
    "                    var_reduction = calculate_variance_reduction(y, subsets_y)\n",
    "\n",
    "                    if var_reduction > best_var_reduction:\n",
    "                        best_var_reduction = var_reduction\n",
    "                        best_split = {\n",
    "                            'feature_idx': feature_idx,\n",
    "                            'threshold': threshold,\n",
    "                            'indices_map': None,\n",
    "                            'gain': var_reduction, # Store var reduction as 'gain'\n",
    "                            'indices_left': indices_left,\n",
    "                            'indices_right': indices_right\n",
    "                        }\n",
    "            \n",
    "            else: # Categorical\n",
    "                subsets_y = []\n",
    "                indices_map = {}\n",
    "                for value in unique_values:\n",
    "                    indices = np.where(X[:, feature_idx] == value)[0]\n",
    "                    if len(indices) > 0:\n",
    "                        indices_map[value] = indices\n",
    "                        subsets_y.append(y[indices])\n",
    "                \n",
    "                if len(subsets_y) < 2:\n",
    "                    continue\n",
    "                \n",
    "                var_reduction = calculate_variance_reduction(y, subsets_y)\n",
    "\n",
    "                if var_reduction > best_var_reduction:\n",
    "                    best_var_reduction = var_reduction\n",
    "                    best_split = {\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'threshold': None,\n",
    "                        'indices_map': indices_map,\n",
    "                        'gain': var_reduction,\n",
    "                        'indices_left': None,\n",
    "                        'indices_right': None\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "\n",
    "def evaluate_model(model_class, X_df, y_s, task_type='classification', max_depth=5):\n",
    "    \"\"\"\n",
    "    Performs 5-fold CV and prints metrics.\n",
    "    Takes pandas DataFrame X_df and pandas Series y_s for easier data handling.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_df):\n",
    "        # Use .iloc to select rows based on integer index\n",
    "        X_train_df = X_df.iloc[train_index]\n",
    "        y_train_s = y_s.iloc[train_index]\n",
    "        X_test_df = X_df.iloc[test_index]\n",
    "        y_test = y_s.iloc[test_index]\n",
    "        \n",
    "        model = model_class(max_depth=max_depth) # Instantiate new model\n",
    "        \n",
    "        # Special preprocessing for ID3\n",
    "        if isinstance(model, MyID3):\n",
    "            # Create copies to avoid SettingWithCopyWarning\n",
    "            X_train_id3 = X_train_df.copy()\n",
    "            X_test_id3 = X_test_df.copy()\n",
    "            \n",
    "            for col in X_train_id3.columns:\n",
    "                if pd.api.types.is_numeric_dtype(X_train_id3[col]):\n",
    "                    # Discretize based on *training* median\n",
    "                    median = X_train_id3[col].median()\n",
    "                    X_train_id3[col] = (X_train_id3[col] > median).astype(str)\n",
    "                    # Apply the same median to the test set\n",
    "                    X_test_id3[col] = (X_test_id3[col] > median).astype(str)\n",
    "            \n",
    "            model.fit(X_train_id3, y_train_s)\n",
    "            y_pred = model.predict(X_test_id3)\n",
    "        \n",
    "        else:\n",
    "            # For C4.5 and Regressor, pass data as-is\n",
    "            model.fit(X_train_df, y_train_s)\n",
    "            y_pred = model.predict(X_test_df)\n",
    "\n",
    "        # Calculate metrics\n",
    "        fold_scores = {}\n",
    "        if task_type == 'classification':\n",
    "            fold_scores['accuracy'] = accuracy_score(y_test, y_pred)\n",
    "            fold_scores['precision'] = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            fold_scores['recall'] = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "            fold_scores['f1'] = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        else: # Regression\n",
    "            fold_scores['mse'] = mean_squared_error(y_test, y_pred)\n",
    "            fold_scores['rmse'] = np.sqrt(fold_scores['mse'])\n",
    "            fold_scores['mae'] = mean_absolute_error(y_test, y_pred)\n",
    "            try:\n",
    "                # R2 can fail if y_test is constant\n",
    "                fold_scores['r2'] = r2_score(y_test, y_pred)\n",
    "            except ValueError:\n",
    "                 fold_scores['r2'] = -np.inf \n",
    "        \n",
    "        scores.append(fold_scores)\n",
    "\n",
    "    # Calculate and print average scores\n",
    "    avg_scores = {}\n",
    "    print(f\"Average Metrics (5-Fold CV) for {model_class.__name__} (max_depth={max_depth}):\")\n",
    "    for metric in scores[0].keys():\n",
    "        avg_scores[metric] = np.mean([s[metric] for s in scores])\n",
    "        print(f\"  - Avg. {metric.upper()}: {avg_scores[metric]:.4f}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "try:\n",
    "    # --- Task 1: playCricket.csv ---\n",
    "    print(\"--- Task 1: playCricket.csv Evaluation (max_depth=3) ---\")\n",
    "    df_play_cricket = pd.read_csv(\"playCricket.csv\").drop(columns=['Day'])\n",
    "    X_cricket = df_play_cricket.drop(columns=['PlayCricket'])\n",
    "    y_cricket = df_play_cricket['PlayCricket']\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluate_model(MyID3, X_cricket, y_cricket, task_type='classification', max_depth=3)\n",
    "    evaluate_model(MyC45, X_cricket, y_cricket, task_type='classification', max_depth=3)\n",
    "    \n",
    "    # Train on full data and print tree\n",
    "    print(\"\\nTraining on full playCricket.csv to show tree structure:\")\n",
    "    id3_cricket = MyID3(max_depth=3)\n",
    "    id3_cricket.fit(X_cricket, y_cricket)\n",
    "    id3_cricket.print_tree()\n",
    "    \n",
    "    c45_cricket = MyC45(max_depth=3)\n",
    "    c45_cricket.fit(X_cricket, y_cricket)\n",
    "    c45_cricket.print_tree()\n",
    "\n",
    "    # --- Task 2: drug_200.csv ---\n",
    "    print(\"\\n--- Task 2: drug_200.csv Evaluation (max_depth=4) ---\")\n",
    "    df_drug = pd.read_csv(\"drug_200.csv\")\n",
    "    X_drug = df_drug.drop(columns=['Drug'])\n",
    "    y_drug = df_drug['Drug']\n",
    "    \n",
    "    # Using max_depth=4 as a compromise for printing\n",
    "    evaluate_model(MyID3, X_drug, y_drug, task_type='classification', max_depth=4)\n",
    "    evaluate_model(MyC45, X_drug, y_drug, task_type='classification', max_depth=4)\n",
    "    \n",
    "    # Train ID3 on full data and print tree\n",
    "    print(\"\\nTraining on full drug_200.csv to show tree structure (max_depth=4):\")\n",
    "    id3_drug = MyID3(max_depth=4)\n",
    "    # Preprocess for ID3\n",
    "    X_drug_id3 = X_drug.copy()\n",
    "    for col in X_drug_id3.columns:\n",
    "        if pd.api.types.is_numeric_dtype(X_drug_id3[col]):\n",
    "            median = X_drug_id3[col].median()\n",
    "            X_drug_id3[col] = (X_drug_id3[col] > median).astype(str)\n",
    "    id3_drug.fit(X_drug_id3, y_drug)\n",
    "    id3_drug.print_tree()\n",
    "\n",
    "    # Train C4.5 on full data and print tree\n",
    "    c45_drug = MyC45(max_depth=4)\n",
    "    c45_drug.fit(X_drug, y_drug)\n",
    "    c45_drug.print_tree()\n",
    "\n",
    "\n",
    "    # --- Task 3: petrol_consumption.csv ---\n",
    "    print(\"\\n--- Task 3: petrol_consumption.csv Evaluation (max_depth=3) ---\")\n",
    "    df_petrol = pd.read_csv(\"petrol_consumption.csv\")\n",
    "    X_petrol = df_petrol.drop(columns=['Petrol_Consumption'])\n",
    "    y_petrol = df_petrol['Petrol_Consumption']\n",
    "    \n",
    "    evaluate_model(MyDecisionTreeRegressor, X_petrol, y_petrol, task_type='regression', max_depth=3)\n",
    "    \n",
    "    # Train on full data and print tree\n",
    "    print(\"\\nTraining on full petrol_consumption.csv to show tree structure (max_depth=3):\")\n",
    "    reg_petrol = MyDecisionTreeRegressor(max_depth=3)\n",
    "    reg_petrol.fit(X_petrol, y_petrol)\n",
    "    reg_petrol.print_tree()\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading file: {e}\")\n",
    "    print(\"Please ensure 'playCricket.csv', 'drug_200.csv', and 'petrol_consumption.csv' are available.\")\n",
    "except Exception as e:\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
