{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a4d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Starting Exploratory Data Analysis (EDA) ---\n",
      "Data loaded successfully for EDA.\n",
      "Generated target_distribution.png\n",
      "Generated numeric_distributions.png\n",
      "Generated duration_vs_target.png\n",
      "Generated job_vs_target.png\n",
      "Generated correlation_heatmap.png\n",
      "Generated categorical_eda_1.png\n",
      "Generated poutcome_vs_target.png\n",
      "Generated balance_vs_target.png\n",
      "--- EDA Finished ---\n",
      "\n",
      "--- 2. Starting 'From Scratch' ML Pipeline ---\n",
      "Data loaded successfully for pipeline.\n",
      "Starting preprocessing...\n",
      "Mapped binary columns to 1/0: ['default', 'housing', 'loan', 'y']\n",
      "One-hot encoded columns: ['job', 'marital', 'education', 'contact', 'month', 'poutcome']\n",
      "Train set shape: (36169, 42), Test set shape: (9042, 42)\n",
      "Scaling numeric features 'from scratch'...\n",
      "Columns to be scaled: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "Scaling applied successfully.\n",
      "\n",
      "--- Training Logistic Regression ---\n",
      "Iteration 0 | Cost: 0.6931\n",
      "Iteration 100 | Cost: 0.4263\n",
      "Iteration 200 | Cost: 0.3529\n",
      "Iteration 300 | Cost: 0.3230\n",
      "Iteration 400 | Cost: 0.3074\n",
      "Iteration 500 | Cost: 0.2978\n",
      "Iteration 600 | Cost: 0.2914\n",
      "Iteration 700 | Cost: 0.2867\n",
      "Iteration 800 | Cost: 0.2831\n",
      "Iteration 900 | Cost: 0.2804\n",
      "Model training complete.\n",
      "\n",
      "--- Evaluating Model ---\n",
      "\n",
      "Confusion Matrix:\n",
      "TP: 114, TN: 7914, FP: 65, FN: 949\n",
      "\n",
      "Metrics:\n",
      "Accuracy:  0.8879\n",
      "Precision: 0.6369\n",
      "Recall:    0.1072\n",
      "F1 Score:  0.1836\n",
      "\n",
      "--- Pipeline Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# --- PART 1: Exploratory Data Analysis (EDA) ---\n",
    "def perform_eda():\n",
    "    print(\"--- 1. Starting Exploratory Data Analysis (EDA) ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(\"bank-full.csv\", sep=';')\n",
    "        print(\"Data loaded successfully for EDA.\")\n",
    "\n",
    "        # 1.1 Target Variable Distribution\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(data=df, x='y')\n",
    "        plt.title('Distribution of Target Variable (y)')\n",
    "        plt.xlabel('Subscribed a Term Deposit')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('target_distribution.png')\n",
    "        plt.close()\n",
    "        print(\"Generated target_distribution.png\")\n",
    "\n",
    "        # 1.2 Numeric Feature Distributions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        sns.histplot(df['age'], bins=30, kde=True, ax=ax1)\n",
    "        ax1.set_title('Distribution of Age')\n",
    "        sns.histplot(df['duration'], bins=50, kde=True, ax=ax2)\n",
    "        ax2.set_title('Distribution of Contact Duration (seconds)')\n",
    "        ax2.set_xlim(0, 2000)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('numeric_distributions.png')\n",
    "        plt.close()\n",
    "        print(\"Generated numeric_distributions.png\")\n",
    "\n",
    "        # 1.3 Duration vs Target\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=df, x='y', y='duration')\n",
    "        plt.title('Contact Duration vs Subscription (y)')\n",
    "        plt.ylim(0, 1500)\n",
    "        plt.savefig('duration_vs_target.png')\n",
    "        plt.close()\n",
    "        print(\"Generated duration_vs_target.png\")\n",
    "\n",
    "        # 1.4 Job vs Target\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.countplot(data=df, x='job', hue='y', order=df['job'].value_counts().index)\n",
    "        plt.title('Job Type vs Subscription (y)')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('job_vs_target.png')\n",
    "        plt.close()\n",
    "        print(\"Generated job_vs_target.png\")\n",
    "\n",
    "        # 1.5 Correlation Heatmap\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        numeric_cols_df = df.select_dtypes(include=np.number)\n",
    "        corr_matrix = numeric_cols_df.corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "        plt.title('Correlation Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('correlation_heatmap.png')\n",
    "        plt.close()\n",
    "        print(\"Generated correlation_heatmap.png\")\n",
    "\n",
    "        # 1.6 Categorical Features vs Target\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        fig.suptitle('Categorical Features vs Subscription (y)', fontsize=16)\n",
    "\n",
    "        sns.countplot(data=df, x='marital', hue='y', ax=axes[0, 0])\n",
    "        sns.countplot(data=df, x='education', hue='y', ax=axes[0, 1])\n",
    "        axes[0, 1].tick_params(axis='x', rotation=30)\n",
    "        sns.countplot(data=df, x='housing', hue='y', ax=axes[1, 0])\n",
    "        sns.countplot(data=df, x='loan', hue='y', ax=axes[1, 1])\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig('categorical_eda_1.png')\n",
    "        plt.close()\n",
    "        print(\"Generated categorical_eda_1.png\")\n",
    "\n",
    "        # 1.7 poutcome vs Target\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.countplot(data=df, x='poutcome', hue='y', order=['unknown', 'failure', 'other', 'success'])\n",
    "        plt.title('Previous Campaign Outcome vs Subscription (y)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('poutcome_vs_target.png')\n",
    "        plt.close()\n",
    "        print(\"Generated poutcome_vs_target.png\")\n",
    "\n",
    "        # 1.8 Balance vs Target\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.boxplot(data=df, x='y', y='balance')\n",
    "        plt.title('Average Yearly Balance vs Subscription (y)')\n",
    "        plt.ylim(-2000, 8000)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('balance_vs_target.png')\n",
    "        plt.close()\n",
    "        print(\"Generated balance_vs_target.png\")\n",
    "\n",
    "        print(\"--- EDA Finished ---\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: bank-full.csv not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during EDA: {e}\")\n",
    "\n",
    "\n",
    "# --- PART 2: Custom Components (From Scratch) ---\n",
    "\n",
    "class MyStandardScaler:\n",
    "    \"\"\"Implements StandardScaler from scratch.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.std_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.std_ = np.std(X, axis=0)\n",
    "        self.std_ = np.where(self.std_ == 0, 1e-8, self.std_)\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.mean_ is None or self.std_ is None:\n",
    "            raise ValueError(\"Fit scaler before transforming.\")\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "def train_test_split_scratch(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    shuffled_indices = np.random.permutation(n_samples)\n",
    "    n_test = int(n_samples * test_size)\n",
    "    n_train = n_samples - n_test\n",
    "\n",
    "    train_indices = shuffled_indices[:n_train]\n",
    "    test_indices = shuffled_indices[n_train:]\n",
    "\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "class MyLogisticRegression:\n",
    "    \"\"\"Logistic Regression from scratch.\"\"\"\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.verbose = verbose\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        z_clipped = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z_clipped))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            A = self._sigmoid(z)\n",
    "            epsilon = 1e-8\n",
    "            cost = -(1/n_samples) * np.sum(y * np.log(A + epsilon) + (1 - y) * np.log(1 - A + epsilon))\n",
    "\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (A - y))\n",
    "            db = (1 / n_samples) * np.sum(A - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            if self.verbose and i % 100 == 0:\n",
    "                print(f\"Iteration {i} | Cost: {cost:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(z)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "\n",
    "\n",
    "# --- Evaluation Metrics ---\n",
    "\n",
    "def confusion_matrix_scratch(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "def accuracy_scratch(y_true, y_pred):\n",
    "    tp, tn, fp, fn = confusion_matrix_scratch(y_true, y_pred)\n",
    "    total = tp + tn + fp + fn\n",
    "    return (tp + tn) / total if total > 0 else 0\n",
    "\n",
    "def precision_scratch(y_true, y_pred):\n",
    "    tp, tn, fp, fn = confusion_matrix_scratch(y_true, y_pred)\n",
    "    return tp / (tp + fp + 1e-8)\n",
    "\n",
    "def recall_scratch(y_true, y_pred):\n",
    "    tp, tn, fp, fn = confusion_matrix_scratch(y_true, y_pred)\n",
    "    return tp / (tp + fn + 1e-8)\n",
    "\n",
    "def f1_score_scratch(y_true, y_pred):\n",
    "    prec = precision_scratch(y_true, y_pred)\n",
    "    rec = recall_scratch(y_true, y_pred)\n",
    "    return 2 * (prec * rec) / (prec + rec + 1e-8)\n",
    "\n",
    "\n",
    "# --- PART 3: Full Pipeline ---\n",
    "\n",
    "def main_pipeline():\n",
    "    print(\"\\n--- 2. Starting 'From Scratch' ML Pipeline ---\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(\"bank-full.csv\", sep=';')\n",
    "        print(\"Data loaded successfully for pipeline.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Starting preprocessing...\")\n",
    "    binary_cols = ['default', 'housing', 'loan', 'y']\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns.drop(binary_cols)\n",
    "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "    for col in binary_cols:\n",
    "        df[col] = df[col].map({'yes': 1, 'no': 0})\n",
    "    print(f\"Mapped binary columns to 1/0: {binary_cols}\")\n",
    "\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "    print(f\"One-hot encoded columns: {categorical_cols.tolist()}\")\n",
    "\n",
    "    X = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "\n",
    "    X_train_df, X_test_df, y_train, y_test = train_test_split_scratch(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_df, columns=X.columns)\n",
    "    X_test_df = pd.DataFrame(X_test_df, columns=X.columns)\n",
    "    print(f\"Train set shape: {X_train_df.shape}, Test set shape: {X_test_df.shape}\")\n",
    "\n",
    "    print(\"Scaling numeric features 'from scratch'...\")\n",
    "    cols_to_scale = [col for col in numeric_cols if col in X_train_df.columns]\n",
    "    print(f\"Columns to be scaled: {cols_to_scale}\")\n",
    "\n",
    "    # ✅ FIX: Ensure all numeric columns are float type\n",
    "    X_train_df[cols_to_scale] = X_train_df[cols_to_scale].apply(pd.to_numeric, errors='coerce')\n",
    "    X_test_df[cols_to_scale] = X_test_df[cols_to_scale].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    scaler = MyStandardScaler()\n",
    "    scaler.fit(X_train_df[cols_to_scale].values)\n",
    "    X_train_df[cols_to_scale] = scaler.transform(X_train_df[cols_to_scale].values)\n",
    "    X_test_df[cols_to_scale] = scaler.transform(X_test_df[cols_to_scale].values)\n",
    "    print(\"Scaling applied successfully.\")\n",
    "\n",
    "# ✅ Convert everything to float before training (fix exp() issue)\n",
    "    X_train_df = X_train_df.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "    X_test_df = X_test_df.apply(pd.to_numeric, errors='coerce').astype(float)\n",
    "\n",
    "    print(\"\\n--- Training Logistic Regression ---\")\n",
    "    model = MyLogisticRegression(learning_rate=0.01, n_iterations=1000, verbose=True)\n",
    "    model.fit(X_train_df.values, y_train.astype(float))  # ensure y is also float\n",
    "    print(\"Model training complete.\")\n",
    "\n",
    "    print(\"\\n--- Evaluating Model ---\")\n",
    "    y_pred = model.predict(X_test_df.values)\n",
    "    tp, tn, fp, fn = confusion_matrix_scratch(y_test, y_pred)\n",
    "    acc = accuracy_scratch(y_test, y_pred)\n",
    "    prec = precision_scratch(y_test, y_pred)\n",
    "    rec = recall_scratch(y_test, y_pred)\n",
    "    f1 = f1_score_scratch(y_test, y_pred)\n",
    "\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall:    {rec:.4f}\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    print(\"\\n--- Pipeline Complete ---\")\n",
    "\n",
    "\n",
    "# --- Run Everything ---\n",
    "if __name__ == \"__main__\":\n",
    "    perform_eda()\n",
    "    main_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
