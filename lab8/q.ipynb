{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5370118f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Performance Comparison (5-Fold CV) =====\n",
      "\n",
      "                           Accuracy  Precision    Recall  F1-score\n",
      "Softmax Logistic (No Reg)     0.935   0.900016  0.944183  0.912367\n",
      "Softmax Lasso (L1)            0.950   0.951770  0.954573  0.947747\n",
      "Softmax Ridge (L2)            0.930   0.902706  0.925072  0.899137\n",
      "Softmax Elastic Net           0.945   0.941361  0.942901  0.937964\n",
      "KNN (K=1)                     0.895   0.871286  0.928733  0.880698\n",
      "KNN (K=3)                     0.830   0.806333  0.831865  0.805048\n",
      "KNN (K=5)                     0.780   0.731445  0.784725  0.742573\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# Load and preprocess data\n",
    "# =========================\n",
    "df = pd.read_csv(\"../lab7/drug_200.csv\")\n",
    "\n",
    "X = df.drop('Drug', axis=1)\n",
    "y = df['Drug']\n",
    "\n",
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# ===============================\n",
    "# Softmax Logistic Regression (Multi-class)\n",
    "# ===============================\n",
    "class SoftmaxRegressionScratch:\n",
    "    def __init__(self, lr=0.1, epochs=1000, reg_type=None, lam=0.01, l1_ratio=0.5):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.reg_type = reg_type\n",
    "        self.lam = lam\n",
    "        self.l1_ratio = l1_ratio\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(np.unique(y))\n",
    "        self.W = np.zeros((n_features, n_classes))\n",
    "        self.b = np.zeros((1, n_classes))\n",
    "\n",
    "        # One-hot encode y\n",
    "        y_onehot = np.zeros((n_samples, n_classes))\n",
    "        y_onehot[np.arange(n_samples), y] = 1\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            logits = np.dot(X, self.W) + self.b\n",
    "            probs = self.softmax(logits)\n",
    "            error = probs - y_onehot\n",
    "\n",
    "            grad_W = (np.dot(X.T, error)) / n_samples\n",
    "            grad_b = np.mean(error, axis=0, keepdims=True)\n",
    "\n",
    "            # Regularization\n",
    "            if self.reg_type == \"l1\":  # Lasso\n",
    "                grad_W += self.lam * np.sign(self.W)\n",
    "            elif self.reg_type == \"l2\":  # Ridge\n",
    "                grad_W += self.lam * self.W\n",
    "            elif self.reg_type == \"elastic\":\n",
    "                grad_W += self.lam * (self.l1_ratio * np.sign(self.W) + (1 - self.l1_ratio) * self.W)\n",
    "\n",
    "            # Update weights\n",
    "            self.W -= self.lr * grad_W\n",
    "            self.b -= self.lr * grad_b\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.W) + self.b\n",
    "        probs = self.softmax(logits)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "# ===============================\n",
    "# KNN Classifier (from scratch)\n",
    "# ===============================\n",
    "class KNNClassifierScratch:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for x in X:\n",
    "            distances = np.sqrt(np.sum((self.X_train - x)**2, axis=1))\n",
    "            k_idx = np.argsort(distances)[:self.k]\n",
    "            k_labels = [self.y_train[i] for i in k_idx]\n",
    "            preds.append(Counter(k_labels).most_common(1)[0][0])\n",
    "        return np.array(preds)\n",
    "\n",
    "# ===============================\n",
    "# Cross-validation evaluation\n",
    "# ===============================\n",
    "def cross_val_evaluate(model_class, X, y, folds=5, **kwargs):\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    accs, precs, recs, f1s = [], [], [], []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        model = model_class(**kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro', zero_division=0)\n",
    "        accs.append(acc)\n",
    "        precs.append(prec)\n",
    "        recs.append(rec)\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return {\n",
    "        \"Accuracy\": np.mean(accs),\n",
    "        \"Precision\": np.mean(precs),\n",
    "        \"Recall\": np.mean(recs),\n",
    "        \"F1-score\": np.mean(f1s)\n",
    "    }\n",
    "\n",
    "# ===============================\n",
    "# Run models\n",
    "# ===============================\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression variants\n",
    "results[\"Softmax Logistic (No Reg)\"] = cross_val_evaluate(SoftmaxRegressionScratch, X, y, reg_type=None)\n",
    "results[\"Softmax Lasso (L1)\"] = cross_val_evaluate(SoftmaxRegressionScratch, X, y, reg_type=\"l1\", lam=0.01)\n",
    "results[\"Softmax Ridge (L2)\"] = cross_val_evaluate(SoftmaxRegressionScratch, X, y, reg_type=\"l2\", lam=0.01)\n",
    "results[\"Softmax Elastic Net\"] = cross_val_evaluate(SoftmaxRegressionScratch, X, y, reg_type=\"elastic\", lam=0.01, l1_ratio=0.5)\n",
    "\n",
    "# KNN variants\n",
    "results[\"KNN (K=1)\"] = cross_val_evaluate(KNNClassifierScratch, X, y, k=1)\n",
    "results[\"KNN (K=3)\"] = cross_val_evaluate(KNNClassifierScratch, X, y, k=3)\n",
    "results[\"KNN (K=5)\"] = cross_val_evaluate(KNNClassifierScratch, X, y, k=5)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n===== Model Performance Comparison (5-Fold CV) =====\\n\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
