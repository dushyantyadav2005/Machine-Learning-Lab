{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053dd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a32bd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
      "0       0  ...         0         0         0         0         0         0   \n",
      "1       0  ...         0         0         0         0         0         0   \n",
      "2       0  ...         0         0         0         0         0         0   \n",
      "3       0  ...         0         0         0         0         0         0   \n",
      "4       0  ...         0         0         0         0         0         0   \n",
      "\n",
      "   pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0  \n",
      "1         0         0         0         0  \n",
      "2         0         0         0         0  \n",
      "3         0         0         0         0  \n",
      "4         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdf=pd.read_csv(\"Assignment-10/MNIST_data/train.csv\")\n",
    "print(mdf.head())\n",
    "mdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb19387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================\n",
      "         Running Part 1: MNIST             \n",
      "===========================================\n",
      "Loading data...\n",
      "train.csv and test.csv loaded.\n",
      "Preparing data...\n",
      "Creating 80/20 train/validation split...\n",
      "Training set size:   33600 samples\n",
      "Validation set size: 8400 samples\n",
      "SoftmaxRegression initialized (lr=0.1, iterations=300)\n",
      "Starting model training...\n",
      "  Iteration 50/300, Loss: 0.8000\n",
      "  Iteration 100/300, Loss: 0.6066\n",
      "  Iteration 150/300, Loss: 0.5287\n",
      "  Iteration 200/300, Loss: 0.4847\n",
      "  Iteration 250/300, Loss: 0.4556\n",
      "  Iteration 300/300, Loss: 0.4347\n",
      "Training complete.\n",
      "Computing metrics on validation set...\n",
      "\n",
      "--- Q1: Softmax Regression Metrics (Validation Set) ---\n",
      "Accuracy:  0.8871\n",
      "Precision (Macro): 0.8861\n",
      "Recall (Macro):    0.8856\n",
      "F1-Score (Macro):  0.8854\n",
      "---------------------------------\n",
      "Predicting labels for test.csv...\n",
      "First 10 predictions for test.csv: [2 0 9 7 3 7 0 3 0 3]\n",
      "\n",
      "--- Demonstrating Helper Functions (as required) ---\n",
      "\n",
      "Demonstrating convert_array_to_image...\n",
      "Saved converted_image.png\n",
      "\n",
      "Demonstrating predict_single_image...\n",
      "Predicting for a single image...\n",
      "Predicted Label: 7\n",
      "Saved predicted_image_label_7.png\n",
      "Function returned: label=7, image_shape=(28, 28)\n",
      "\n",
      "===========================================\n",
      "         Running Part 2: Bank Data         \n",
      "===========================================\n",
      "Loading data...\n",
      "bank-full.csv loaded.\n",
      "Preparing data...\n",
      "Creating 80/20 train/test split (as per assumption)...\n",
      "Training set size: 36168 samples\n",
      "Test set size:     9043 samples\n",
      "MixedNaiveBayes initialized (Laplace smoothing alpha=1)\n",
      "Starting model training...\n",
      "  Found 7 numerical features.\n",
      "  Found 9 categorical features.\n",
      "Training complete.\n",
      "Computing metrics on test set...\n",
      "\n",
      "--- Q2: Naive Bayes Metrics (Test Set) ---\n",
      "Accuracy:  0.8886\n",
      "Precision (Macro): 0.7319\n",
      "Recall (Macro):    0.7359\n",
      "F1-Score (Macro):  0.7339\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "# Suppress warnings (e.g., from log(0)) for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# PART 0: EVALUATION METRICS (FROM SCRATCH)\n",
    "#\n",
    "# These functions will be used for both Part 1 and Part 2.\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes accuracy from scratch.\n",
    "    \"\"\"\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    total_samples = len(y_true)\n",
    "    return correct_predictions / total_samples\n",
    "\n",
    "def precision_recall_f1_score(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    Computes Precision, Recall, and F1-Score from scratch.\n",
    "    \n",
    "    Supports 'macro' averaging for multi-class (and binary) problems.\n",
    "    \"\"\"\n",
    "    # Get all unique classes from true labels\n",
    "    classes = np.unique(y_true)\n",
    "    \n",
    "    # Dictionaries to store metrics for each class\n",
    "    precisions = {}\n",
    "    recalls = {}\n",
    "    f1_scores = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        # True Positives (TP): True label is c, Predicted label is c\n",
    "        tp = np.sum((y_true == c) & (y_pred == c))\n",
    "        \n",
    "        # False Positives (FP): True label is NOT c, Predicted label is c\n",
    "        fp = np.sum((y_true != c) & (y_pred == c))\n",
    "        \n",
    "        # False Negatives (FN): True label is c, Predicted label is NOT c\n",
    "        fn = np.sum((y_true == c) & (y_pred != c))\n",
    "        \n",
    "        # Calculate precision, recall, and f1 for class c\n",
    "        precision_c = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall_c = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        if (precision_c + recall_c) > 0:\n",
    "            f1_c = 2 * (precision_c * recall_c) / (precision_c + recall_c)\n",
    "        else:\n",
    "            f1_c = 0\n",
    "            \n",
    "        precisions[c] = precision_c\n",
    "        recalls[c] = recall_c\n",
    "        f1_scores[c] = f1_c\n",
    "        \n",
    "    if average == 'macro':\n",
    "        # Calculate the mean of the metrics across all classes\n",
    "        avg_precision = np.mean(list(precisions.values()))\n",
    "        avg_recall = np.mean(list(recalls.values()))\n",
    "        avg_f1 = np.mean(list(f1_scores.values()))\n",
    "        \n",
    "        return avg_precision, avg_recall, avg_f1\n",
    "    else:\n",
    "        # We could implement 'micro' or 'weighted' here, but 'macro' is sufficient\n",
    "        raise ValueError(\"Only 'macro' average is supported.\")\n",
    "\n",
    "def print_evaluation_metrics(y_true, y_pred, title=\"Evaluation Metrics\"):\n",
    "    \"\"\"\n",
    "    Helper function to print all metrics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- {title} ---\")\n",
    "    \n",
    "    # Note: The assignment PDF listed 'Accuracy' twice.\n",
    "    # We will compute Accuracy, Precision, Recall, and F1-Score.\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1 = precision_recall_f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"Precision (Macro): {precision:.4f}\")\n",
    "    print(f\"Recall (Macro):    {recall:.4f}\")\n",
    "    print(f\"F1-Score (Macro):  {f1:.4f}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# PART 1: LOGISTIC (SOFTMAX) REGRESSION ON MNIST\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "class SoftmaxRegression:\n",
    "    \"\"\"\n",
    "    Implements Softmax Regression (Multinomial Logistic Regression) from scratch.\n",
    "    \n",
    "    This is for multi-class classification (e.g., MNIST digits 0-9).\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.1, n_iterations=300, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        np.random.seed(random_state) # for reproducible weights\n",
    "        print(f\"SoftmaxRegression initialized (lr={learning_rate}, iterations={n_iterations})\")\n",
    "\n",
    "    def _one_hot(self, y, n_classes):\n",
    "        \"\"\"Helper to one-hot encode the target labels.\"\"\"\n",
    "        # Creates a matrix of zeros with shape (n_samples, n_classes)\n",
    "        # and sets the column corresponding to the class label to 1.\n",
    "        y_hot = np.zeros((y.shape[0], n_classes))\n",
    "        y_hot[np.arange(y.shape[0]), y] = 1\n",
    "        return y_hot\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        \"\"\"Computes the softmax activation function.\"\"\"\n",
    "        # Subtract max(z) for numerical stability (prevents overflow)\n",
    "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "    def _cross_entropy_loss(self, y_true_hot, y_pred_proba):\n",
    "        \"\"\"Computes the categorical cross-entropy loss.\"\"\"\n",
    "        # Add a small epsilon to avoid log(0)\n",
    "        epsilon = 1e-9\n",
    "        n_samples = y_true_hot.shape[0]\n",
    "        loss = - (1 / n_samples) * np.sum(y_true_hot * np.log(y_pred_proba + epsilon))\n",
    "        return loss\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trains the model using gradient descent.\"\"\"\n",
    "        print(\"Starting model training...\")\n",
    "        \n",
    "        # 1. Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Initialize weights (n_features, n_classes) and bias (1, n_classes)\n",
    "        self.weights = np.random.randn(n_features, self.n_classes) * 0.01\n",
    "        self.bias = np.zeros((1, self.n_classes))\n",
    "        \n",
    "        # 2. One-hot encode the true labels\n",
    "        y_true_hot = self._one_hot(y, self.n_classes)\n",
    "        \n",
    "        # 3. Gradient Descent\n",
    "        for i in range(self.n_iterations):\n",
    "            # Calculate scores (logits)\n",
    "            z = X.dot(self.weights) + self.bias\n",
    "            \n",
    "            # Calculate predicted probabilities\n",
    "            y_pred_proba = self._softmax(z)\n",
    "            \n",
    "            # Calculate the gradient of the loss w.r.t. scores (z)\n",
    "            # Gradient = (Predicted Probas - True Labels (hot))\n",
    "            dz = y_pred_proba - y_true_hot\n",
    "            \n",
    "            # Calculate gradients for weights and bias\n",
    "            # dW = (1/N) * X.T dot dz\n",
    "            # db = (1/N) * sum(dz)\n",
    "            dw = (1 / n_samples) * X.T.dot(dz)\n",
    "            db = (1 / n_samples) * np.sum(dz, axis=0, keepdims=True)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                loss = self._cross_entropy_loss(y_true_hot, y_pred_proba)\n",
    "                print(f\"  Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        print(\"Training complete.\")\n",
    "        self.model_params = {'weights': self.weights, 'bias': self.bias}\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Makes probability predictions for new data.\"\"\"\n",
    "        if self.weights is None or self.bias is None:\n",
    "            raise Exception(\"Model not trained yet. Call fit() first.\")\n",
    "        z = X.dot(self.weights) + self.bias\n",
    "        return self._softmax(z)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes class predictions for new data.\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        # Return the class with the highest probability\n",
    "        return np.argmax(probabilities, axis=1)\n",
    "\n",
    "# --- Q1: Required Helper Functions ---\n",
    "\n",
    "def convert_array_to_image(arr, display=True):\n",
    "    \"\"\"\n",
    "    Converts a 784-element array into a 28x28 image and optionally displays it.\n",
    "    \"\"\"\n",
    "    # Reshape the 1D array (784) into a 2D array (28, 28)\n",
    "    image_array = arr.reshape(28, 28)\n",
    "    \n",
    "    if display:\n",
    "        plt.imshow(image_array, cmap='gray')\n",
    "        plt.title(\"Converted Image\")\n",
    "        plt.axis('off')\n",
    "        # Save the plot to a file\n",
    "        plt.savefig(\"converted_image.png\")\n",
    "        print(\"Saved converted_image.png\")\n",
    "        plt.close() # Close the plot to avoid showing it inline\n",
    "    \n",
    "    # Return the 2D array, which can be considered the \"image\"\n",
    "    return image_array\n",
    "\n",
    "def predict_single_image(array_784, model):\n",
    "    \"\"\"\n",
    "    Takes a 784-array and a trained model, and returns\n",
    "    an image (2D array) and its predicted label.\n",
    "    \"\"\"\n",
    "    print(\"Predicting for a single image...\")\n",
    "    \n",
    "    # 1. Normalize the pixel data (as done during training)\n",
    "    arr_normalized = array_784 / 255.0\n",
    "    \n",
    "    # 2. Reshape for the model (1 sample, 784 features)\n",
    "    arr_reshaped = arr_normalized.reshape(1, 784)\n",
    "    \n",
    "    # 3. Get the predicted label from the model\n",
    "    predicted_label = model.predict(arr_reshaped)[0]\n",
    "    \n",
    "    # 4. Convert the original array to an image (2D array)\n",
    "    # We use the original non-normalized array for visualization\n",
    "    image_2d = array_784.reshape(28, 28)\n",
    "    \n",
    "    print(f\"Predicted Label: {predicted_label}\")\n",
    "    \n",
    "    # Display the image with its prediction\n",
    "    plt.imshow(image_2d, cmap='gray')\n",
    "    plt.title(f\"Predicted Label: {predicted_label}\")\n",
    "    plt.axis('off')\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(f\"predicted_image_label_{predicted_label}.png\")\n",
    "    print(f\"Saved predicted_image_label_{predicted_label}.png\")\n",
    "    plt.close() # Close the plot to avoid showing it inline\n",
    "    \n",
    "    return image_2d, predicted_label\n",
    "\n",
    "# --- Q1: Main Execution ---\n",
    "\n",
    "def run_part_1():\n",
    "    print(\"===========================================\")\n",
    "    print(\"         Running Part 1: MNIST             \")\n",
    "    print(\"===========================================\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # Load from uploaded files\n",
    "        train_df = pd.read_csv(\"Assignment-10/MNIST_data/train.csv\")\n",
    "        test_df = pd.read_csv(\"Assignment-10/MNIST_data/test.csv\")\n",
    "        print(\"train.csv and test.csv loaded.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: train.csv or test.csv not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare Data\n",
    "    print(\"Preparing data...\")\n",
    "    \n",
    "    # Separate labels (y) from features (X) in the training data\n",
    "    y_train_full = train_df['label'].values\n",
    "    X_train_full = train_df.drop('label', axis=1).values\n",
    "    \n",
    "    # The provided test.csv has no labels.\n",
    "    X_test_final = test_df.values\n",
    "\n",
    "    # Normalize pixel values (from 0-255 to 0-1)\n",
    "    X_train_full = X_train_full / 255.0\n",
    "    X_test_final = X_test_final / 255.0\n",
    "\n",
    "    # 3. Create Validation Split\n",
    "    # We must split train.csv to get metrics, as test.csv has no labels.\n",
    "    # We use a fixed random_state for reproducible results.\n",
    "    print(\"Creating 80/20 train/validation split...\")\n",
    "    indices = np.arange(X_train_full.shape[0])\n",
    "    np.random.seed(42) # for reproducibility\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_point = int(0.8 * len(indices))\n",
    "    train_idx, val_idx = indices[:split_point], indices[split_point:]\n",
    "    \n",
    "    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "\n",
    "    print(f\"Training set size:   {X_train.shape[0]} samples\")\n",
    "    print(f\"Validation set size: {X_val.shape[0]} samples\")\n",
    "\n",
    "    # 4. Train the Model\n",
    "    # Note: Training from scratch is slow. 300 iterations is a good compromise.\n",
    "    # For higher accuracy, increase n_iterations (e.g., to 1000).\n",
    "    model = SoftmaxRegression(learning_rate=0.1, n_iterations=300)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Compute Evaluation Metrics on Validation Set\n",
    "    print(\"Computing metrics on validation set...\")\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    print_evaluation_metrics(y_val, y_pred_val, title=\"Q1: Softmax Regression Metrics (Validation Set)\")\n",
    "\n",
    "    # 6. Predict on the 'test.csv' file\n",
    "    print(\"Predicting labels for test.csv...\")\n",
    "    final_predictions = model.predict(X_test_final)\n",
    "    print(f\"First 10 predictions for test.csv: {final_predictions[:10]}\")\n",
    "\n",
    "    # 7. Demonstrate required helper functions\n",
    "    print(\"\\n--- Demonstrating Helper Functions (as required) ---\")\n",
    "    \n",
    "    # Pick a sample from the test set to demonstrate\n",
    "    sample_index = 5\n",
    "    sample_array = test_df.iloc[sample_index].values\n",
    "    \n",
    "    # a) Demonstrate convert_array_to_image\n",
    "    print(\"\\nDemonstrating convert_array_to_image...\")\n",
    "    _ = convert_array_to_image(sample_array, display=True)\n",
    "    \n",
    "    # b) Demonstrate predict_single_image\n",
    "    print(\"\\nDemonstrating predict_single_image...\")\n",
    "    # Get the already trained model\n",
    "    trained_model = model\n",
    "    # Use the raw array (non-normalized) as input, function will handle it\n",
    "    image, label = predict_single_image(sample_array, trained_model)\n",
    "    print(f\"Function returned: label={label}, image_shape={image.shape}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "# PART 2: NAIVE BAYES ON BANK DATA\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "class MixedNaiveBayes:\n",
    "    \"\"\"\n",
    "    Implements Naive Bayes from scratch.\n",
    "    \n",
    "    Handles mixed data types:\n",
    "    - Gaussian distribution for numerical features.\n",
    "    - Categorical distribution for categorical features.\n",
    "    \"\"\"\n",
    "    def __init__(self, laplace=1):\n",
    "        self.laplace = laplace # for Laplace/additive smoothing\n",
    "        self.classes = None\n",
    "        self.class_priors = {}\n",
    "        self.likelihoods = {} # For categorical features\n",
    "        self.gaussian_params = {} # For numerical features\n",
    "        self.categorical_cols = None\n",
    "        self.numerical_cols = None\n",
    "        print(f\"MixedNaiveBayes initialized (Laplace smoothing alpha={laplace})\")\n",
    "\n",
    "    def _gaussian_pdf(self, x, mean, std):\n",
    "        \"\"\"Calculates the Gaussian Probability Density Function.\"\"\"\n",
    "        # Add a small epsilon to std to prevent division by zero\n",
    "        epsilon = 1e-9\n",
    "        std = std + epsilon\n",
    "        exponent = np.exp(-((x - mean) ** 2) / (2 * std ** 2))\n",
    "        return (1 / (np.sqrt(2 * np.pi) * std)) * exponent\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Trains the Naive Bayes model.\"\"\"\n",
    "        print(\"Starting model training...\")\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Identify column types from the DataFrame\n",
    "        self.categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        self.numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        print(f\"  Found {len(self.numerical_cols)} numerical features.\")\n",
    "        print(f\"  Found {len(self.categorical_cols)} categorical features.\")\n",
    "        \n",
    "        for c in self.classes:\n",
    "            # 1. Calculate Class Priors: P(y)\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[c] = X_c.shape[0] / n_samples\n",
    "            \n",
    "            # 2. Calculate Gaussian Parameters (Mean, Std) for numerical features\n",
    "            self.gaussian_params[c] = {}\n",
    "            for col in self.numerical_cols:\n",
    "                mean = X_c[col].mean()\n",
    "                std = X_c[col].std()\n",
    "                self.gaussian_params[c][col] = (mean, std)\n",
    "                \n",
    "            # 3. Calculate Likelihoods for categorical features: P(x_i | y)\n",
    "            self.likelihoods[c] = {}\n",
    "            for col in self.categorical_cols:\n",
    "                counts = X_c[col].value_counts()\n",
    "                total_count = X_c.shape[0]\n",
    "                \n",
    "                # Get all unique values for this feature from the *entire* dataset\n",
    "                # This is for the denominator in Laplace smoothing\n",
    "                n_unique_values = len(X[col].unique())\n",
    "                \n",
    "                # Store probabilities for each value in this feature\n",
    "                self.likelihoods[c][col] = {}\n",
    "                for val, count in counts.items():\n",
    "                    # Apply Laplace Smoothing\n",
    "                    self.likelihoods[c][col][val] = (count + self.laplace) / (total_count + (n_unique_values * self.laplace))\n",
    "                \n",
    "                # Handle values that might be in other classes but not this one\n",
    "                # This probability is for any value in the test set that wasn't\n",
    "                # seen for this class 'c' during training.\n",
    "                self.likelihoods[c][col]['_unknown_'] = self.laplace / (total_count + (n_unique_values * self.laplace))\n",
    "\n",
    "        print(\"Training complete.\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Makes predictions for new data.\"\"\"\n",
    "        \n",
    "        # *********** THE FIX IS HERE ***********\n",
    "        # Changed 'if not self.classes:' to 'if self.classes is None:'\n",
    "        # This correctly checks if the model has been trained.\n",
    "        if self.classes is None:\n",
    "            raise Exception(\"Model not trained yet. Call fit() first.\")\n",
    "        \n",
    "        y_pred = []\n",
    "        \n",
    "        # Iterate over each sample (row) in the test set\n",
    "        for _, row in X.iterrows():\n",
    "            posteriors = {}\n",
    "            \n",
    "            # Calculate posterior probability for each class\n",
    "            for c in self.classes:\n",
    "                # Start with the log of the class prior\n",
    "                # We use logs to prevent numerical underflow (multiplying many small probs)\n",
    "                log_posterior = np.log(self.class_priors[c])\n",
    "                \n",
    "                # Add log-likelihood for numerical features\n",
    "                for col in self.numerical_cols:\n",
    "                    mean, std = self.gaussian_params[c][col]\n",
    "                    prob = self._gaussian_pdf(row[col], mean, std)\n",
    "                    # Add small epsilon to avoid log(0)\n",
    "                    log_posterior += np.log(prob + 1e-9)\n",
    "                    \n",
    "                # Add log-likelihood for categorical features\n",
    "                for col in self.categorical_cols:\n",
    "                    val = row[col]\n",
    "                    # Get prob, defaulting to the '_unknown_' prob if value wasn't seen\n",
    "                    prob = self.likelihoods[c][col].get(val, self.likelihoods[c][col]['_unknown_'])\n",
    "                    log_posterior += np.log(prob)\n",
    "                \n",
    "                posteriors[c] = log_posterior\n",
    "            \n",
    "            # The class with the highest log-posterior is the prediction\n",
    "            y_pred.append(max(posteriors, key=posteriors.get))\n",
    "            \n",
    "        return np.array(y_pred)\n",
    "\n",
    "# --- Q2: Main Execution ---\n",
    "\n",
    "def run_part_2():\n",
    "    print(\"\\n===========================================\")\n",
    "    print(\"         Running Part 2: Bank Data         \")\n",
    "    print(\"===========================================\")\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"Loading data...\")\n",
    "    try:\n",
    "        # The bank-full.csv file uses semicolons as delimiters\n",
    "        data = pd.read_csv(\"Assignment-10/bank-full.csv\", sep=';')\n",
    "        print(\"bank-full.csv loaded.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: bank-full.csv not found.\")\n",
    "        return\n",
    "\n",
    "    # 2. Prepare Data\n",
    "    print(\"Preparing data...\")\n",
    "    # Convert target 'y' to numerical (0 or 1)\n",
    "    # This makes it easier for our from-scratch metrics\n",
    "    data['y'] = data['y'].map({'no': 0, 'yes': 1})\n",
    "    \n",
    "    X = data.drop('y', axis=1)\n",
    "    y = data['y']\n",
    "\n",
    "    # 3. Create Train/Test Split\n",
    "    # *** ASSUMPTION ***\n",
    "    # As Assignment 9 is not available, we create a standard 80/20 split.\n",
    "    # We use random_state=42 for reproducibility.\n",
    "    print(\"Creating 80/20 train/test split (as per assumption)...\")\n",
    "    \n",
    "    # Simple split without scikit-learn\n",
    "    # We shuffle the data first\n",
    "    data_shuffled = data.sample(frac=1, random_state=42)\n",
    "    split_point = int(0.8 * len(data_shuffled))\n",
    "    \n",
    "    train_data = data_shuffled[:split_point]\n",
    "    test_data = data_shuffled[split_point:]\n",
    "    \n",
    "    X_train = train_data.drop('y', axis=1)\n",
    "    y_train = train_data['y']\n",
    "    X_test = test_data.drop('y', axis=1)\n",
    "    y_test = test_data['y']\n",
    "\n",
    "    print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set size:     {X_test.shape[0]} samples\")\n",
    "\n",
    "    # 4. Train the Model\n",
    "    nb_model = MixedNaiveBayes(laplace=1)\n",
    "    nb_model.fit(X_train, y_train)\n",
    "\n",
    "    # 5. Compute Evaluation Metrics on Test Set\n",
    "    print(\"Computing metrics on test set...\")\n",
    "    y_pred_test = nb_model.predict(X_test)\n",
    "    \n",
    "    # Use the same from-scratch metrics from Part 0\n",
    "    print_evaluation_metrics(y_test, y_pred_test, title=\"Q2: Naive Bayes Metrics (Test Set)\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "#\n",
    "#                           MAIN EXECUTION\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "# We need to wrap the main execution in a way that can be run\n",
    "# by the code interpreter.\n",
    "try:\n",
    "    # Run Part 1 (MNIST)\n",
    "    run_part_1()\n",
    "    \n",
    "    # Run Part 2 (Bank Data)\n",
    "    run_part_2()\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nExecution failed: {e}\")\n",
    "    print(\"Please ensure 'train.csv', 'test.csv', and 'bank-full.csv' are uploaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
